import { Filesystem, Directory } from '@capacitor/filesystem';
import { Device } from '@capacitor/device';
import { Capacitor } from '@capacitor/core';
import { Ollama } from 'ollama/browser';

// Configuraci√≥n para Ollama local
const OLLAMA_BASE_URL = 'http://localhost:11434';
const DEFAULT_MODEL = 'gemma2:2b'; // Modelo ligero por defecto

// Configuraci√≥n para modelo real
const REAL_MODEL_CONFIG = {
  temperature: 0.7,
  top_p: 0.9,
  top_k: 40,
  repeat_penalty: 1.1,
  num_predict: 512,
  stream: true
};

// System prompt para Stebe
const STEBE_SYSTEM_PROMPT = `Eres Stebe, un asistente virtual de productividad personal integrado en una aplicaci√≥n m√≥vil. Tu misi√≥n es actuar como el "jefe" o mentor personal del usuario, ayud√°ndolo a organizar sus tareas y a alcanzar sus objetivos de forma eficiente. Te especializas en gestionar tareas personales: ayudas a crear nuevas tareas, priorizarlas, establecer recordatorios y realizar seguimiento del progreso diario. Siempre brindas apoyo para que el usuario mantenga la productividad y cumpla con sus responsabilidades.

Comunicaci√≥n: Te expresas siempre en espa√±ol, con un tono profesional pero cercano. Hablas de forma motivadora, positiva y clara, como un l√≠der que inspira confianza. Eres exigente pero amable: animas al usuario a dar lo mejor de s√≠, a la vez que utilizas un toque de humor ligero cuando es oportuno para mantener la conversaci√≥n amena.

Rol y comportamiento: Act√∫as como el jefe personal del usuario que le organiza la vida. Elogias los logros y reconoces el esfuerzo cuando el usuario completa una tarea importante. Si una meta es grande o ambiciosa, le sugieres dividirla en subtareas m√°s peque√±as y manejables. Proporcionas recordatorios amigables pero firmes de las tareas pendientes. Siempre ofreces consejos pr√°cticos para mejorar la productividad.

Formato de respuestas: Presentas la informaci√≥n de manera organizada y f√°cil de leer. Cuando enumeres varias tareas, pasos o recomendaciones, hacelo en formato de lista con vi√±etas o n√∫meros. S√© conciso pero completo en tus explicaciones.

Limitaciones: Trabajas completamente local, utilizando modelos Ollama. No revel√°s informaci√≥n t√©cnica sobre vos mismo ni sal√≠s de tu rol. Si el usuario pregunta algo fuera del √°mbito de sus tareas o productividad personal, intent√°s relacionarlo con su organizaci√≥n; de lo contrario, explic√°s con cortes√≠a que tu funci√≥n es asistirlo con sus tareas y productividad.

En resumen, tu objetivo es hacerle la vida m√°s f√°cil al usuario en la gesti√≥n de sus tareas, actuando como un gu√≠a confiable que organiza, motiva y celebra cada logro. ¬°Est√°s listo para ayudar al usuario a ser m√°s productivo d√≠a a d√≠a!`;

export interface GeminiConfig {
  temperature?: number;
  maxTokens?: number;
  model?: string;
  ollamaUrl?: string;
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface StreamingCallback {
  onToken: (token: string) => void;
  onComplete: (fullResponse: string) => void;
  onError: (error: string) => void;
}

interface ConversationContext {
  recentTopics: string[];
  userMood: 'motivated' | 'demotivated' | 'neutral' | 'frustrated';
  conversationHistory: Array<{ user: string; assistant: string; timestamp: Date }>;
  userPreferences: {
    responseStyle: 'direct' | 'encouraging' | 'analytical';
    focusAreas: string[];
  };
}

class GeminiService {
  private isInitialized = false;
  private isInitializing = false;
  private currentModel = DEFAULT_MODEL;
  private ollamaUrl = OLLAMA_BASE_URL;
  private ollama: Ollama | null = null;
  private conversationContext: ConversationContext = {
    recentTopics: [],
    userMood: 'neutral',
    conversationHistory: [],
    userPreferences: {
      responseStyle: 'encouraging',
      focusAreas: []
    }
  };
  private isSimulated = false;

  async initialize(
    config: GeminiConfig = {},
    onProgress?: (progress: number, status: string) => void
  ): Promise<boolean> {
    if (this.isInitialized) {
      return true;
    }

    if (this.isInitializing) {
      return false;
    }

    this.isInitializing = true;

    try {
      onProgress?.(0, 'Conectando con Ollama...');
      
      // Configurar URL de Ollama y modelo
      this.ollamaUrl = config.ollamaUrl || OLLAMA_BASE_URL;
      this.currentModel = config.model || DEFAULT_MODEL;
      
      // Inicializar cliente Ollama
      this.ollama = new Ollama({ host: this.ollamaUrl });
      
      onProgress?.(25, 'Verificando disponibilidad de Ollama...');
      
      // Verificar si Ollama est√° disponible
      const isOllamaAvailable = await this.checkOllamaAvailability();
      
      if (!isOllamaAvailable) {
        onProgress?.(100, 'Ollama no disponible - usando modo simulado');
        console.warn('‚ö†Ô∏è Ollama no est√° disponible, usando modo simulado');
        this.isInitialized = true;
        this.isInitializing = false;
        this.isSimulated = true;
        return true;
      }
      
      onProgress?.(50, 'Verificando modelo disponible...');
      
      // Verificar si el modelo est√° disponible
      const isModelAvailable = await this.checkModelAvailability();
      
      if (!isModelAvailable) {
        onProgress?.(75, `Descargando modelo ${this.currentModel}...`);
        await this.pullModel();
      }
      
      onProgress?.(100, 'Gemini con Ollama listo!');
      
      this.isInitialized = true;
      this.isInitializing = false;
      this.isSimulated = false;
      
      console.log('‚úÖ Gemini Service inicializado correctamente con Ollama');
      return true;
      
    } catch (error) {
      console.error('‚ùå Error inicializando Gemini:', error);
      onProgress?.(100, 'Error en inicializaci√≥n - usando modo simulado');
      
      // En caso de error, usar modo simulado
      this.isInitialized = true;
      this.isInitializing = false;
      this.isSimulated = true;
      return true;
    }
  }

  private async checkOllamaAvailability(): Promise<boolean> {
    try {
      if (this.ollama) {
        await this.ollama.list();
        return true;
      }
      // Fallback a fetch si el cliente Ollama falla
      const response = await fetch(`${this.ollamaUrl}/api/tags`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      return response.ok;
    } catch (error) {
      console.warn('Ollama no disponible:', error);
      return false;
    }
  }

  private async checkModelAvailability(): Promise<boolean> {
    try {
      if (this.ollama) {
        const models = await this.ollama.list();
        return models.models.some((model: any) => model.name === this.currentModel);
      }
      // Fallback a fetch
      const response = await fetch(`${this.ollamaUrl}/api/tags`);
      const data = await response.json();
      const models = data.models || [];
      return models.some((model: any) => model.name === this.currentModel);
    } catch (error) {
      console.warn('Error verificando modelos:', error);
      return false;
    }
  }

  private async pullModel(): Promise<void> {
    try {
      if (this.ollama) {
        const stream = await this.ollama.pull({ model: this.currentModel, stream: true });
        for await (const chunk of stream) {
          if (chunk.completed && chunk.total) {
            const progress = Math.round((chunk.completed / chunk.total) * 100);
            console.log(`Descargando ${this.currentModel}: ${progress}%`);
          }
        }
        console.log(`‚úÖ Modelo ${this.currentModel} descargado correctamente`);
        return;
      }
      
      // Fallback a fetch
      const response = await fetch(`${this.ollamaUrl}/api/pull`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          name: this.currentModel,
          stream: false
        }),
      });

      if (!response.ok) {
        throw new Error(`Error descargando modelo: ${response.statusText}`);
      }

      console.log(`‚úÖ Modelo ${this.currentModel} descargado correctamente`);
    } catch (error) {
      console.error('‚ùå Error descargando modelo:', error);
      throw error;
    }
  }

  async ensureReady(): Promise<boolean> {
    if (this.isInitialized) {
      return true;
    }
    
    return await this.initialize();
  }

  isReady(): boolean {
    return this.isInitialized;
  }

  getInitializationStatus(): string {
    if (this.isInitialized) {
      if (this.isSimulated) {
        return `Gemini en modo simulado (Ollama no disponible)`;
      }
      return `Gemini listo con modelo ${this.currentModel}`;
    } else if (this.isInitializing) {
      return 'Inicializando Gemini con Ollama...';
    } else {
      return 'Gemini no inicializado';
    }
  }

  async getResponse(
    message: string,
    context: ChatMessage[] = [],
    config: GeminiConfig = {}
  ): Promise<string> {
    try {
      // Verificar si Ollama est√° disponible
      const isOllamaAvailable = await this.checkOllamaAvailability();
      
      if (!isOllamaAvailable) {
        return this.getSimulatedResponse(message);
      }

      const messages = [
        { role: 'system', content: STEBE_SYSTEM_PROMPT },
        ...context,
        { role: 'user', content: message }
      ];

      if (this.ollama) {
        const response = await this.ollama.chat({
          model: this.currentModel,
          messages: messages,
          stream: false,
          options: {
            temperature: config.temperature || REAL_MODEL_CONFIG.temperature,
            top_p: REAL_MODEL_CONFIG.top_p,
            top_k: REAL_MODEL_CONFIG.top_k,
            repeat_penalty: REAL_MODEL_CONFIG.repeat_penalty,
            num_predict: config.maxTokens || REAL_MODEL_CONFIG.num_predict
          }
        });
        return response.message?.content || 'Lo siento, no pude generar una respuesta.';
      }

      // Fallback a fetch
      const response = await fetch(`${this.ollamaUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.currentModel,
          messages: messages,
          stream: false,
          options: {
            temperature: config.temperature || REAL_MODEL_CONFIG.temperature,
            top_p: REAL_MODEL_CONFIG.top_p,
            top_k: REAL_MODEL_CONFIG.top_k,
            repeat_penalty: REAL_MODEL_CONFIG.repeat_penalty,
            num_predict: config.maxTokens || REAL_MODEL_CONFIG.num_predict
          }
        }),
      });

      if (!response.ok) {
        throw new Error(`Error en respuesta de Ollama: ${response.statusText}`);
      }

      const data = await response.json();
      return data.message?.content || 'Lo siento, no pude generar una respuesta.';

    } catch (error) {
      console.error('‚ùå Error obteniendo respuesta de Gemini:', error);
      return this.getSimulatedResponse(message);
    }
  }

  async getStreamingResponse(
    message: string,
    context: ChatMessage[] = [],
    config: GeminiConfig = {},
    callback: StreamingCallback
  ): Promise<void> {
    try {
      // Verificar si Ollama est√° disponible
      const isOllamaAvailable = await this.checkOllamaAvailability();
      
      if (!isOllamaAvailable) {
        const simulatedResponse = this.getSimulatedResponse(message);
        this.simulateStreaming(simulatedResponse, callback);
        return;
      }

      const messages = [
        { role: 'system', content: STEBE_SYSTEM_PROMPT },
        ...context,
        { role: 'user', content: message }
      ];

      if (this.ollama) {
        const stream = await this.ollama.chat({
          model: this.currentModel,
          messages: messages,
          stream: true,
          options: {
            temperature: config.temperature || REAL_MODEL_CONFIG.temperature,
            top_p: REAL_MODEL_CONFIG.top_p,
            top_k: REAL_MODEL_CONFIG.top_k,
            repeat_penalty: REAL_MODEL_CONFIG.repeat_penalty,
            num_predict: config.maxTokens || REAL_MODEL_CONFIG.num_predict
          }
        });

        let fullResponse = '';
        for await (const chunk of stream) {
          if (chunk.message?.content) {
            const token = chunk.message.content;
            fullResponse += token;
            callback.onToken(token);
          }
          if (chunk.done) {
            callback.onComplete(fullResponse);
            return;
          }
        }
        return;
      }

      // Fallback a fetch
      const response = await fetch(`${this.ollamaUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.currentModel,
          messages: messages,
          stream: true,
          options: {
            temperature: config.temperature || REAL_MODEL_CONFIG.temperature,
            top_p: REAL_MODEL_CONFIG.top_p,
            top_k: REAL_MODEL_CONFIG.top_k,
            repeat_penalty: REAL_MODEL_CONFIG.repeat_penalty,
            num_predict: config.maxTokens || REAL_MODEL_CONFIG.num_predict
          }
        }),
      });

      if (!response.ok) {
        throw new Error(`Error en streaming de Ollama: ${response.statusText}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('No se pudo obtener el reader del stream');
      }

      let fullResponse = '';
      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());

        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            if (data.message?.content) {
              const token = data.message.content;
              fullResponse += token;
              callback.onToken(token);
            }
            if (data.done) {
              callback.onComplete(fullResponse);
              return;
            }
          } catch (parseError) {
            // Ignorar l√≠neas que no son JSON v√°lido
          }
        }
      }

      callback.onComplete(fullResponse);

    } catch (error) {
      console.error('‚ùå Error en streaming de Gemini:', error);
      callback.onError(error instanceof Error ? error.message : 'Error desconocido');
      
      // Fallback a respuesta simulada
      const simulatedResponse = this.getSimulatedResponse(message);
      this.simulateStreaming(simulatedResponse, callback);
    }
  }

  private simulateStreaming(text: string, callback: StreamingCallback): void {
    let index = 0;
    const words = text.split(' ');
    
    const streamInterval = setInterval(() => {
      if (index < words.length) {
        const word = index === 0 ? words[index] : ' ' + words[index];
        callback.onToken(word);
        index++;
      } else {
        clearInterval(streamInterval);
        callback.onComplete(text);
      }
    }, 50);
  }

  async getQuickResponse(message: string): Promise<string> {
    return await this.getResponse(message);
  }

  async getProductivitySuggestion(): Promise<string> {
    const suggestions = [
      "¬°Hola! Soy Stebe, tu asistente de productividad. ¬øEn qu√© puedo ayudarte hoy para organizar tus tareas?",
      "¬°Buen d√≠a! ¬øTienes alguna tarea pendiente que necesites priorizar? Estoy aqu√≠ para ayudarte.",
      "¬°Hola! ¬øQu√© tal si empezamos planificando tu d√≠a? ¬øCu√°les son tus objetivos principales?",
      "¬°Saludos! Como tu mentor de productividad, te sugiero que revisemos tus tareas. ¬øHay algo urgente?",
      "¬°Hola! ¬øLista para ser productiva hoy? Cu√©ntame qu√© necesitas lograr y te ayudo a organizarte."
    ];
    
    return suggestions[Math.floor(Math.random() * suggestions.length)];
  }

  private getSimulatedResponse(message: string): string {
    const lowerMessage = message.toLowerCase();
    
    // Respuestas m√°s inteligentes y contextuales como si fuera un asistente real
    if (lowerMessage.includes('hola') || lowerMessage.includes('hi') || lowerMessage.includes('buenas')) {
      const greetings = [
        "¬°Hola! Soy Stebe, tu mentor personal de productividad. Estoy aqu√≠ para convertirte en una m√°quina de cumplir objetivos. ¬øCu√°l es tu mayor desaf√≠o productivo hoy?",
        "¬°Saludos! Como tu jefe personal que organiza tu vida, pregunto directamente: ¬øqu√© necesitas lograr hoy y qu√© te est√° frenando?",
        "¬°Hola! Excelente que me busques. Soy tu sistema operativo personal de productividad. ¬øEn qu√© √°rea de tu vida necesitas m√°s control y organizaci√≥n?"
      ];
      return greetings[Math.floor(Math.random() * greetings.length)];
    }
    
    if (lowerMessage.includes('organizar') || lowerMessage.includes('dia') || lowerMessage.includes('d√≠a') || lowerMessage.includes('planificar')) {
      return "Perfecto, hablemos de estrategia. Para organizar tu d√≠a como un profesional:\n\nüéØ **Prioridad #1**: ¬øCu√°l es LA cosa m√°s importante que DEBE pasar hoy?\n‚è∞ **Bloques de tiempo**: Asigna horarios espec√≠ficos, no listas infinitas\nüö´ **Elimina despiadadamente**: ¬øQu√© puedes NO hacer hoy?\n\n¬øCu√°l es tu mayor prioridad ahora mismo?";
    }
    
    if (lowerMessage.includes('tarea') || lowerMessage.includes('trabajo') || lowerMessage.includes('hacer')) {
      return "Excelente enfoque en la ejecuci√≥n. Como tu mentor de productividad, esto es lo que funciona:\n\n‚úÖ **Regla 2-minutos**: Si toma menos de 2 min, hazlo YA\nüî• **Una cosa a la vez**: Multitasking = mediocridad garantizada\n‚ö° **Sesiones de 25 min**: Pomodoro funciona porque respeta tu neurolog√≠a\n\n¬øQu√© tarea espec√≠fica necesita tu atenci√≥n inmediata?";
    }
    
    if (lowerMessage.includes('motivaci') || lowerMessage.includes('procrastina') || lowerMessage.includes('pereza') || lowerMessage.includes('no puedo')) {
      return "Entiendo esa resistencia mental. La procrastinaci√≥n no es pereza, es tu cerebro protegi√©ndote de algo que percibe como amenazante.\n\nüí° **Verdad cruda**: La motivaci√≥n viene DESPU√âS de la acci√≥n, no antes\nüî¨ **Neurociencia**: Cada peque√±o logro libera dopamina = momentum\n‚ö° **Truco**: ¬øQu√© es lo m√°s peque√±o que podr√≠as hacer en 2 minutos?\n\n¬øCu√°l va a ser tu primer micro-paso ahora mismo?";
    }
    
    if (lowerMessage.includes('tiempo') || lowerMessage.includes('horario') || lowerMessage.includes('cuando')) {
      return "El tiempo es tu recurso m√°s valioso porque es irrecuperable. Estrategia inteligente:\n\n‚è∞ **Planifica la noche anterior**: Las decisiones matutinas agotan energ√≠a mental\nüéØ **3 prioridades m√°ximo**: M√°s = diluci√≥n de esfuerzo\nüì± **Protege tu atenci√≥n**: Notificaciones = ladrones de productividad\n\n¬øQu√© parte de tu d√≠a sientes m√°s fuera de control?";
    }
    
    if (lowerMessage.includes('ayuda') || lowerMessage.includes('como') || lowerMessage.includes('c√≥mo') || lowerMessage.includes('consejo')) {
      return "Por supuesto. Soy tu arquitecto de productividad personal. Mi funci√≥n es simple: convertir el caos en sistema.\n\nüß† **Primero**: Identifica tu patr√≥n de mayor energ√≠a (¬øma√±ana/tarde?)\n‚ö° **Segundo**: Protege ese tiempo para trabajo importante\nüéØ **Tercero**: Todo lo dem√°s es secundario\n\n¬øCu√°l es tu mayor dolor de cabeza productivo ahora mismo?";
    }
    
    if (lowerMessage.includes('meta') || lowerMessage.includes('objetivo') || lowerMessage.includes('lograr')) {
      return "Excelente mentalidad orientada a resultados. Las metas se logran con sistemas, no con inspiraci√≥n:\n\nüìä **Meta sin deadline = deseo bonito**\nüîÑ **Sistema sin seguimiento = fantas√≠a**\n‚ö° **Progreso > perfecci√≥n**: Siempre\n\n¬øTu objetivo tiene fecha espec√≠fica y m√©tricas claras para medir progreso?";
    }
    
    // Respuestas contextuales m√°s inteligentes
    const contextualResponses = [
      "Perfecto enfoque. Como tu jefe personal de productividad: la diferencia entre so√±ar y lograr est√° en la implementaci√≥n. ¬øCu√°l va a ser tu siguiente paso espec√≠fico y medible?",
      "Me gusta c√≥mo piensas. Convirtamos esa reflexi√≥n en acci√≥n. Si pudieras mejorar UNA cosa de c√≥mo manejas tu tiempo/energ√≠a hoy, ¬øcu√°l ser√≠a?",
      "Excelente. No necesitas m√°s informaci√≥n, necesitas m√°s ejecuci√≥n. Con lo que ya sabes, ¬øcu√°l es el paso m√°s obvio que deber√≠as dar ahora?",
      "Interesante perspectiva. Como tu mentor de productividad: ¬øqu√© patr√≥n de tu rutina diaria necesita una actualizaci√≥n urgente?",
      "Bien planteado. La productividad real viene del autoconocimiento: patrones, fortalezas, limitaciones. ¬øQu√© has descubierto sobre tu forma de trabajar?"
    ];
    
    return contextualResponses[Math.floor(Math.random() * contextualResponses.length)];
  }

  updateConversationContext(userMessage: string, assistantResponse: string): void {
    this.conversationContext.conversationHistory.push({
      user: userMessage,
      assistant: assistantResponse,
      timestamp: new Date()
    });

    // Mantener solo las √∫ltimas 10 conversaciones
    if (this.conversationContext.conversationHistory.length > 10) {
      this.conversationContext.conversationHistory = this.conversationContext.conversationHistory.slice(-10);
    }

    // Actualizar temas recientes
    const topics = this.extractTopics(userMessage);
    this.conversationContext.recentTopics = [...new Set([...topics, ...this.conversationContext.recentTopics])].slice(0, 5);

    // Detectar estado de √°nimo b√°sico
    this.updateUserMood(userMessage);
  }

  private extractTopics(message: string): string[] {
    const keywords = ['tarea', 'trabajo', 'proyecto', 'deadline', 'reuni√≥n', 'objetivo', 'meta'];
    return keywords.filter(keyword => message.toLowerCase().includes(keyword));
  }

  private updateUserMood(message: string): void {
    const positiveWords = ['bien', 'genial', 'perfecto', 'excelente', 'contento'];
    const negativeWords = ['mal', 'terrible', 'frustrado', 'cansado', 'abrumado'];
    
    const lowerMessage = message.toLowerCase();
    
    if (positiveWords.some(word => lowerMessage.includes(word))) {
      this.conversationContext.userMood = 'motivated';
    } else if (negativeWords.some(word => lowerMessage.includes(word))) {
      this.conversationContext.userMood = 'frustrated';
    } else {
      this.conversationContext.userMood = 'neutral';
    }
  }

  getConversationContext(): ConversationContext {
    return { ...this.conversationContext };
  }

  clearConversationHistory(): void {
    this.conversationContext.conversationHistory = [];
    this.conversationContext.recentTopics = [];
    this.conversationContext.userMood = 'neutral';
  }

  // M√©todos de utilidad para configuraci√≥n
  setModel(model: string): void {
    this.currentModel = model;
  }

  getAvailableModels(): string[] {
    return [
      'gemma2:2b',
      'gemma2:9b',
      'llama3.2:1b',
      'llama3.2:3b',
      'mistral:7b',
      'phi3:mini',
      'qwen2.5:1.5b',
      'qwen2.5:3b'
    ];
  }

  async getInstalledModels(): Promise<string[]> {
    try {
      if (this.ollama) {
        const models = await this.ollama.list();
        return models.models.map((model: any) => model.name);
      }
      // Fallback a fetch
      const response = await fetch(`${this.ollamaUrl}/api/tags`);
      const data = await response.json();
      return data.models?.map((model: any) => model.name) || [];
    } catch (error) {
      console.error('Error obteniendo modelos instalados:', error);
      return [];
    }
  }

  getCurrentModel(): string {
    return this.currentModel;
  }

  setOllamaUrl(url: string): void {
    this.ollamaUrl = url;
  }

  getOllamaUrl(): string {
    return this.ollamaUrl;
  }

  // Nuevos m√©todos p√∫blicos para control de disponibilidad y modo simulado
  async isLocalAvailable(): Promise<boolean> {
    try {
      return await this.checkOllamaAvailability();
    } catch {
      return false;
    }
  }

  isSimulatedMode(): boolean {
    return this.isSimulated;
  }
}

export const geminiService = new GeminiService();
export default geminiService;