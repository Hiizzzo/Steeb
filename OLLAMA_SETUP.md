# Configuraci√≥n de Ollama para STEBE AI

Esta gu√≠a te ayudar√° a configurar Ollama localmente para usar modelos de IA con STEBE AI.

## üöÄ Instalaci√≥n de Ollama

### Windows
1. Descarga el instalador desde [ollama.ai](https://ollama.ai/download)
2. Ejecuta el instalador y sigue las instrucciones
3. Ollama se instalar√° como servicio y estar√° disponible en `http://localhost:11434`

### macOS
```bash
# Usando Homebrew
brew install ollama

# O descarga desde ollama.ai
curl -fsSL https://ollama.ai/install.sh | sh
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## üì¶ Modelos Recomendados

### Modelos Ligeros (Recomendados para empezar)
```bash
# Gemma 2B - Muy r√°pido, ideal para respuestas b√°sicas
ollama pull gemma2:2b

# Llama 3.2 1B - Extremadamente ligero
ollama pull llama3.2:1b

# Phi3 Mini - Optimizado para tareas espec√≠ficas
ollama pull phi3:mini
```

### Modelos Medianos (Mejor calidad)
```bash
# Llama 3.2 3B - Buen balance entre velocidad y calidad
ollama pull llama3.2:3b

# Gemma 9B - Excelente para espa√±ol
ollama pull gemma2:9b

# Qwen 3B - Muy bueno para productividad
ollama pull qwen2.5:3b
```

### Modelos Avanzados (Requieren m√°s recursos)
```bash
# Mistral 7B - Excelente calidad general
ollama pull mistral:7b

# Llama 3.1 8B - Muy completo
ollama pull llama3.1:8b
```

## ‚öôÔ∏è Configuraci√≥n en STEBE AI

### 1. Verificar que Ollama est√° ejecut√°ndose
```bash
# Verificar estado
ollama list

# Si no est√° ejecut√°ndose, iniciarlo
ollama serve
```

### 2. Configurar modelo en la aplicaci√≥n
El servicio Gemini est√° configurado para usar `gemma2:2b` por defecto. Puedes cambiarlo:

```typescript
// En el componente StebeAI
const config: GeminiConfig = {
  temperature: 0.7,
  maxTokens: 1024,
  model: 'gemma2:2b' // Cambia aqu√≠ el modelo
};
```

### 3. Modelos disponibles desde la UI
La aplicaci√≥n detectar√° autom√°ticamente los modelos instalados y permitir√° cambiar entre ellos.

## üîß Configuraci√≥n Avanzada

### Cambiar puerto de Ollama
```bash
# Cambiar puerto por defecto
export OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

Luego actualizar en la aplicaci√≥n:
```typescript
const config: GeminiConfig = {
  ollamaUrl: 'http://localhost:11435',
  model: 'gemma2:2b'
};
```

### Optimizaci√≥n de rendimiento
```bash
# Variables de entorno para mejor rendimiento
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_FLASH_ATTENTION=1
```

## üìä Requisitos del Sistema

### M√≠nimos (Modelos 1B-2B)
- **RAM**: 4GB libres
- **CPU**: Cualquier procesador moderno
- **Espacio**: 2-4GB por modelo

### Recomendados (Modelos 3B-7B)
- **RAM**: 8GB libres
- **CPU**: 4+ n√∫cleos
- **Espacio**: 5-15GB por modelo

### √ìptimos (Modelos 8B+)
- **RAM**: 16GB+ libres
- **CPU**: 8+ n√∫cleos o GPU
- **Espacio**: 15-30GB por modelo

## üö¶ Verificaci√≥n de Funcionamiento

### 1. Test b√°sico de Ollama
```bash
# Verificar que Ollama responde
curl http://localhost:11434/api/tags

# Test r√°pido de chat
curl http://localhost:11434/api/generate -d '{
  "model": "gemma2:2b",
  "prompt": "Hola, ¬øc√≥mo est√°s?",
  "stream": false
}'
```

### 2. Test desde STEBE AI
1. Abrir la aplicaci√≥n
2. Ir a la secci√≥n de STEBE AI
3. Hacer clic en "Inicializar"
4. Deber√≠a mostrar "Gemini con Ollama listo!"

## ‚ùó Soluci√≥n de Problemas

### Ollama no se conecta
```bash
# Verificar que est√° ejecut√°ndose
ps aux | grep ollama

# Reiniciar servicio
pkill ollama
ollama serve

# Verificar puerto
netstat -tlnp | grep 11434
```

### Modelo no encontrado
```bash
# Listar modelos instalados
ollama list

# Instalar modelo faltante
ollama pull gemma2:2b
```

### Respuestas lentas
1. Usar modelos m√°s peque√±os (1B-2B)
2. Reducir `maxTokens` en la configuraci√≥n
3. Cerrar otras aplicaciones que usen mucha RAM

### Error de memoria
```bash
# Verificar memoria disponible
free -h

# Usar modelo m√°s peque√±o o aumentar RAM virtual
ollama pull llama3.2:1b
```

## üéØ Configuraci√≥n √ìptima para STEBE AI

Para la mejor experiencia con STEBE AI, recomendamos:

```bash
# Instalar modelo optimizado para espa√±ol
ollama pull gemma2:2b

# Configuraci√≥n recomendada
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_LOADED_MODELS=1
```

### Configuraci√≥n en la aplicaci√≥n:
```typescript
const config: GeminiConfig = {
  temperature: 0.7,      // Creatividad balanceada
  maxTokens: 512,        // Respuestas concisas
  model: 'gemma2:2b'     // Modelo optimizado
};
```

## üìù Comandos √ötiles

```bash
# Ver modelos disponibles online
ollama search gemma

# Eliminar modelo no usado
ollama rm llama3.1:8b

# Ver uso de recursos
ollama ps

# Actualizar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

## üîó Enlaces √ötiles

- [Ollama Official Website](https://ollama.ai)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Lista completa de modelos](https://ollama.ai/library)
- [Documentaci√≥n API](https://github.com/ollama/ollama/blob/main/docs/api.md)

---

Con esta configuraci√≥n, STEBE AI funcionar√° completamente offline usando modelos locales de IA, proporcionando respuestas r√°pidas y personalizadas para gesti√≥n de productividad.