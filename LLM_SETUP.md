# üöÄ LLM.js Setup - Stebe AI sin dependencias externas

## ¬øQu√© es LLM.js?

**@themaximalist/llm.js** es una librer√≠a universal que ya est√° en tu proyecto.

‚úÖ **Interfaz √∫nica** para todos los LLMs  
‚úÖ **Soporta Ollama** (local, gratis, SIN API keys)  
‚úÖ **Tambi√©n soporta** OpenAI, Google, Anthropic  
‚úÖ **Ya instalado** en tu proyecto  

---

## üéØ Lo que obtuviste

### Componente
- `src/components/SteebChatLLM.tsx` - Chat UI completo

### Servicio
- `src/services/llmService.ts` - Integraci√≥n con LLM.js

---

## ‚ö° Uso r√°pido

### Opci√≥n 1: Ollama (Recomendado - GRATIS y LOCAL)

```tsx
import SteebChatLLM from '@/components/SteebChatLLM';

export default function App() {
  return <SteebChatLLM />;
}
```

Cuando abras el componente:
1. Ver√°s 4 opciones de LLM
2. Elige **Ollama** (recomendado)
3. Aseg√∫rate que Ollama est√° corriendo localmente
4. ¬°Listo!

### Opci√≥n 2: Servicio directo

```tsx
import llmService from '@/services/llmService';

// Inicializar con Ollama
const initialized = await llmService.initialize({
  provider: 'ollama',
  baseUrl: 'http://localhost:11434',
  model: 'mistral'
});

// Usar
const response = await llmService.sendMessage('Hola Steeb');
```

---

## üìã Requisitos por provider

### Ollama (Recomendado)
```bash
# 1. Instala Ollama desde https://ollama.ai
# 2. Abre terminal y ejecuta:
ollama run mistral

# ¬°Listo! Se ejecuta en http://localhost:11434
```

**Ventajas:**
- ‚úÖ Completamente gratis
- ‚úÖ Totalmente privado (local)
- ‚úÖ Sin API keys
- ‚úÖ Funciona offline
- ‚úÖ Muy r√°pido

**Desventajas:**
- ‚ùå Necesita instalar Ollama en tu m√°quina
- ‚ùå Usa recursos locales

---

### OpenAI
1. Ve a https://platform.openai.com/api/keys
2. Crea una API key
3. P√©gala en el chat

**Ventajas:**
- ‚úÖ Muy potente (GPT-4, GPT-3.5)
- ‚úÖ R√°pido

**Desventajas:**
- ‚ùå Requiere pago (cr√©dito)
- ‚ùå Requiere API key

---

### Google Gemini
1. Ve a https://makersuite.google.com/app/apikey
2. Crea una API key gratis
3. P√©gala en el chat

**Ventajas:**
- ‚úÖ Gratis (limited)
- ‚úÖ Muy potente

**Desventajas:**
- ‚ùå Limitaciones de cuota
- ‚ùå Requiere Google account

---

### Anthropic Claude
1. Ve a https://console.anthropic.com
2. Crea una API key
3. P√©gala en el chat

**Ventajas:**
- ‚úÖ Excelente calidad

**Desventajas:**
- ‚ùå Requiere pago
- ‚ùå Precio m√°s alto

---

## üéØ Configuraci√≥n con Ollama

### Paso 1: Instalar Ollama

Descarga desde: https://ollama.ai

### Paso 2: Ejecutar un modelo

```bash
# Ejecutar en terminal
ollama run mistral
```

O elige otro modelo:
```bash
ollama run llama2
ollama run neural-chat
ollama run orca-mini
ollama run openchat
```

### Paso 3: Verificar conexi√≥n

Abre en navegador:
```
http://localhost:11434/api/tags
```

Deber√≠as ver tus modelos en JSON.

### Paso 4: Usar en Stebe

1. Abre `SteebChatLLM`
2. Elige **Ollama**
3. Verifica que `http://localhost:11434` es correcto
4. Elige el modelo que ejecutaste
5. ¬°Chat!

---

## üìñ M√©todos disponibles

### Chat simple
```tsx
const response = await llmService.sendMessage('Tu mensaje');
```

### Analizar intenci√≥n
```tsx
const analysis = await llmService.analyzeUserMessage(
  'Necesito aprender React'
);
// Responde: { intent, extractedTasks, priority }
```

### Motivaci√≥n
```tsx
const motivation = await llmService.getMotivationalResponse({
  tasksPending: 5,
  userMood: 'tired'
});
```

### Generar plan
```tsx
const plan = await llmService.generateTaskPlan(
  'Quiero aprender TypeScript'
);
// Responde: { tasks, motivation, nextSteps }
```

### Verificar estado
```tsx
const ready = llmService.isReady();
const info = llmService.getProviderInfo();
// { provider, ready, requiresApiKey }
```

### Cambiar provider
```tsx
await llmService.switchProvider({
  provider: 'openai',
  apiKey: 'tu-api-key',
  model: 'gpt-3.5-turbo'
});
```

---

## ‚úÖ Checklist r√°pido

### Para Ollama
- [ ] Instale Ollama desde ollama.ai
- [ ] Ejecut√© `ollama run mistral` en terminal
- [ ] Import√© `SteebChatLLM` en mi app
- [ ] Elij√≠ Ollama en el chat
- [ ] ¬°Funciona!

### Para OpenAI / Google
- [ ] Obtuve mi API key
- [ ] Import√© `SteebChatLLM`
- [ ] Elij√≠ el provider
- [ ] Pegu√© mi key
- [ ] ¬°Funciona!

---

## üÜò Troubleshooting

### "Connection refused" en Ollama
**Soluci√≥n:**
- Verifica que ejecutaste `ollama run mistral`
- Verifica que Ollama est√° corriendo
- Intenta `http://localhost:11434` en navegador

### "API key inv√°lida"
**Soluci√≥n:**
- Copia la key sin espacios
- Verifica que sea la key correcta
- Intenta crear una nueva key

### "Respuesta lenta"
**Soluci√≥n:**
- Si es Ollama, intenta modelo m√°s peque√±o: `orca-mini`
- Si es OpenAI, el servidor puede estar saturado
- Intenta de nuevo despu√©s

### "Modelo no encontrado"
**Soluci√≥n:**
- Para Ollama: ejecuta `ollama pull mistral` primero
- Para otros: verifica el nombre del modelo

---

## üß™ Testear los m√©todos

```tsx
import llmService from '@/services/llmService';

// En una funci√≥n
const test = async () => {
  // Inicializar
  await llmService.initialize({ provider: 'ollama' });
  
  // Test 1: Chat simple
  const msg = await llmService.sendMessage('Hola');
  console.log('Test 1:', msg);
  
  // Test 2: An√°lisis
  const analysis = await llmService.analyzeUserMessage('Tengo 3 tareas');
  console.log('Test 2:', analysis);
  
  // Test 3: Motivaci√≥n
  const mot = await llmService.getMotivationalResponse();
  console.log('Test 3:', mot);
  
  // Test 4: Plan
  const plan = await llmService.generateTaskPlan('Aprender React');
  console.log('Test 4:', plan);
};

test();
```

---

## üí° Tips

### Para mejor performance
1. Usa Ollama con modelo peque√±o (`orca-mini`)
2. O usa OpenAI (m√°s r√°pido en general)
3. Limpia contexto si tarda: `llmService.clearContext()`

### Para privacidad total
1. Usa Ollama (todo local)
2. No necesitas internet
3. Tus datos nunca salen de tu m√°quina

### Para mejor calidad
1. Usa `mistral` o `neural-chat` en Ollama
2. O usa OpenAI GPT-4
3. O usa Anthropic Claude

---

## üöÄ Pr√≥ximos pasos

1. **Elige tu provider:**
   - Ollama (LOCAL, GRATIS) ‚Üê Recomendado
   - OpenAI (Potente, costo bajo)
   - Google (Gratis limited)

2. **Configura:**
   - Ollama: `ollama run mistral`
   - Otros: Obt√©n tu API key

3. **Usa en tu app:**
   ```tsx
   <SteebChatLLM />
   ```

4. **¬°Elimina procrastinaci√≥n!** üî•

---

## üìö Modelos disponibles

### Ollama (Local)
- `mistral` - Muy bueno, balance
- `llama2` - Cl√°sico, muy usado
- `neural-chat` - Optimizado para chat
- `orca-mini` - Peque√±o, r√°pido
- `openchat` - Open source potente

### OpenAI
- `gpt-4` - Mejor, m√°s caro
- `gpt-3.5-turbo` - Balance precio/calidad

### Google
- `gemini-pro` - Potente, gratis (limited)

### Anthropic
- `claude-3-opus` - Mejor, m√°s caro
- `claude-3-sonnet` - Balance

---

## üéä ¬°Listo!

Tu Stebe AI con LLM.js est√° configurado.

**Pr√≥ximo paso:** Abre tu app y disfruta del chat.

---

**M√°s info:** https://llmjs.themaximalist.com/
**Ollama:** https://ollama.ai
